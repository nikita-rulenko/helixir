<p align="center">
  <img src="helixir-logo.jpeg" alt="Helixir Logo" width="400"/>
</p>

<h1 align="center">ðŸ§  Helixir</h1>

<p align="center">
  <strong>The Fastest Memory for LLM Agents</strong><br/>
  <em>Ontological memory framework for AI assistants</em>
</p>

<p align="center">
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-features">Features</a> â€¢
  <a href="#-mcp-integration">MCP Integration</a> â€¢
  <a href="#-configuration">Configuration</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/rust-1.75+-orange.svg" alt="Rust"/>
  <img src="https://img.shields.io/badge/license-AGPL--3.0-blue.svg" alt="License"/>
  <img src="https://img.shields.io/badge/MCP-compatible-green.svg" alt="MCP"/>
</p>

---

## What is Helixir?

**Helixir** is an associative & causal AI memory framework â€” the fastest way to give your AI agents persistent, structured, reasoning-capable memory.

It gives your AI agents **persistent, structured, reasoning-capable memory**. Instead of losing context between sessions, your AI remembers facts, learns preferences, tracks goals, and builds knowledge over time.

Built on [HelixDB](https://github.com/HelixDB/helix-db) graph-vector database with native [MCP](https://spec.modelcontextprotocol.io/) support for seamless integration with **Cursor**, **Claude Desktop**, and other AI assistants.

### âš¡ Recommended Stack: Cerebras + OpenRouter

For **maximum speed**, use:
- **[Cerebras](https://cloud.cerebras.ai)** for LLM inference â€” 70x faster than GPU, free tier available
- **[OpenRouter](https://openrouter.ai)** for embeddings â€” cheap, reliable, many models

This combination delivers **sub-second memory operations** with the 70B parameter Llama 3.3 model.

### ðŸ¦€ Why Rust?

- âš¡ **~50ms startup** â€” instant response
- ðŸ“¦ **~15MB memory** â€” lightweight footprint
- ðŸŽ¯ **Single binary** â€” zero runtime dependencies
- ðŸ›¡ï¸ **Memory safe** â€” no crashes, no leaks

---

## âœ¨ Features

- **ðŸ”¬ Atomic Fact Extraction** â€” LLM-powered decomposition into atomic facts
- **ðŸ§¹ Smart Deduplication** â€” ADD / UPDATE / SUPERSEDE / NOOP decision engine  
- **ðŸ•¸ï¸ Graph Memory** â€” Entities, relations, and reasoning chains
- **ðŸ” Semantic Search** â€” Vector similarity + graph traversal (SmartTraversalV2)
- **â° Temporal Filtering** â€” recent (4h), contextual (30d), deep (90d), full
- **ðŸ·ï¸ Ontology Mapping** â€” skill, preference, goal, fact, opinion, experience, achievement
- **ðŸ“¡ MCP Server** â€” Native integration with AI assistants
- **ðŸ§© Semantic Chunking** â€” Automatic splitting of long texts
- **ðŸ§  FastThink** â€” In-memory working memory for complex reasoning (scratchpad)
- **ðŸŽ¯ Cognitive Protocol** â€” Built-in triggers and filters that shape AI behavior

---

## ðŸŽ¯ Cognitive Protocol

Helixir-RS is more than memory storage â€” it actively shapes how your AI thinks.

### Automatic Recall Triggers

The AI automatically recalls context when it detects patterns in your message:

| You say | AI does |
|---------|---------|
| "remember", "recall" | Searches recent memory |
| "we discussed", "last time" | Deep search in history |
| "why did we" | Retrieves reasoning chains |
| "what's next", "plan" | Recalls task context |
| "like before" | Looks up preferences |

### Importance Filter

Not everything should be saved. Built-in heuristics keep memory clean:

| Save | Skip |
|------|------|
| Decisions, outcomes | Search/grep results |
| Architecture details | Compiler output |
| Errors and fixes | Temporary debug data |
| User preferences | Duplicate information |

### The Result

Your AI develops consistent habits: recalls context at session start, saves important decisions, uses structured reasoning for complex problems, and builds knowledge over time.

---

## ðŸš€ Quick Start

### One-Command Setup (Docker)

```bash
# Clone and start everything
git clone https://github.com/nikita-rulenko/Helixir
cd helixir-rs

# Create config
cat > .env << 'EOF'
HELIX_LLM_API_KEY=your_cerebras_or_openai_key
HELIX_EMBEDDING_API_KEY=your_openrouter_or_openai_key
EOF

# Start HelixDB + deploy schema
docker-compose up -d
```

**Requirements:**
- Docker & Docker Compose installed
- API keys (see [Configuration](#-configuration))

### Manual Installation

```bash
# 1. Download binary for your platform
# Linux x86_64
curl -L https://github.com/nikita-rulenko/Helixir/releases/latest/download/helixir-linux-x86_64.tar.gz | tar xz

# macOS Apple Silicon
curl -L https://github.com/nikita-rulenko/Helixir/releases/latest/download/helixir-macos-arm64.tar.gz | tar xz

# macOS Intel
curl -L https://github.com/nikita-rulenko/Helixir/releases/latest/download/helixir-macos-x86_64.tar.gz | tar xz

# 2. Start HelixDB (if not running)
docker run -d -p 6969:6969 helixdb/helixdb:latest

# 3. Deploy schema
./helixir-deploy --host localhost --port 6969

# 4. Run MCP server
export LLM_API_KEY=your_key
export EMBEDDING_API_KEY=your_key
./helixir-mcp
```

### Build from Source

```bash
git clone https://github.com/nikita-rulenko/Helixir
cd helixir-rs

# Build
cargo build --release

# Deploy schema & run
./target/release/helixir-deploy --host localhost --port 6969
./target/release/helixir-mcp
```

---

## ðŸ”§ MCP Integration

### Cursor IDE

Edit `~/.cursor/mcp.json`:

```json
{
  "mcpServers": {
    "helixir": {
      "command": "/path/to/helixir-mcp",
      "env": {
        "HELIX_HOST": "localhost",
        "HELIX_PORT": "6969",
        "HELIX_LLM_PROVIDER": "cerebras",
        "HELIX_LLM_MODEL": "llama-3.3-70b",
        "HELIX_LLM_API_KEY": "YOUR_API_KEY",
        "HELIX_EMBEDDING_PROVIDER": "openai",
        "HELIX_EMBEDDING_URL": "https://openrouter.ai/api/v1",
        "HELIX_EMBEDDING_API_KEY": "YOUR_API_KEY"
      }
    }
  }
}
```

### Claude Desktop

**macOS:** `~/Library/Application Support/Claude/claude_desktop_config.json`  
**Windows:** `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "helixir": {
      "command": "/path/to/helixir-mcp",
      "env": {
        "HELIX_HOST": "localhost",
        "HELIX_PORT": "6969",
        "HELIX_LLM_API_KEY": "YOUR_API_KEY",
        "HELIX_EMBEDDING_API_KEY": "YOUR_API_KEY"
      }
    }
  }
}
```

### Cursor Rules (Important!)

To make your AI assistant actually USE the memory, add these rules to **Cursor Settings â†’ Rules**:

```
# Core Memory Behavior
- At conversation start, call search_memory to recall relevant context
- Always use Helixir MCP first to recall context about the current project
- After completing tasks, save key outcomes with add_memory
- After reaching context window limit (when Cursor summarizes), read your role and goals from memory

# Search Strategy
- For memory search, use appropriate mode:
  - "recent" for quick context (last 4 hours)
  - "contextual" for balanced search (30 days)
  - "deep" for thorough search (90 days)
  - "full" for complete history
- Use search_by_concept for skill/preference/goal queries
- Use search_reasoning_chain for "why" questions and logical connections

# FastThink for Complex Reasoning
- Before major decisions, use FastThink to structure your reasoning
- Flow: think_start â†’ think_add (multiple thoughts) â†’ think_recall (get context) â†’ think_conclude â†’ think_commit
- Use think_recall to pull relevant facts from main memory into your thinking session
- If session times out, partial thoughts are auto-saved â€” continue with search_incomplete_thoughts

# What to Save
- ALWAYS save: decisions, outcomes, architecture changes, error fixes
- NEVER save: grep results, lint output, temporary data
```

---

## ðŸ“š MCP Tools

### Memory Operations

| Tool | Description |
|------|-------------|
| `add_memory` | Add memory with LLM extraction â†’ `{memories_added, entities, relations, chunks_created}` |
| `search_memory` | Smart search: `recent` (4h), `contextual` (30d), `deep` (90d), `full` |
| `search_by_concept` | Filter by type: `skill`, `goal`, `preference`, `fact`, `opinion`, `experience`, `achievement` |
| `search_reasoning_chain` | Find logical connections: `IMPLIES`, `BECAUSE`, `CONTRADICTS` |
| `get_memory_graph` | Visualize memory as nodes and edges |
| `update_memory` | Update existing memory content |

### FastThink (Working Memory)

| Tool | Description |
|------|-------------|
| `think_start` | Start isolated thinking session â†’ `{session_id, root_thought_idx}` |
| `think_add` | Add thought to session â†’ `{thought_idx, thought_count, depth}` |
| `think_recall` | Recall facts from main memory (read-only) â†’ `{recalled_count, thought_indices}` |
| `think_conclude` | Mark conclusion â†’ `{conclusion_idx, status: "decided"}` |
| `think_commit` | Save conclusion to main memory â†’ `{memory_id, thoughts_processed}` |
| `think_discard` | Discard session without saving â†’ `{discarded_thoughts}` |
| `think_status` | Get session status â†’ `{thought_count, depth, has_conclusion, elapsed_ms}` |

### Usage Examples

**Store a preference:**
```
"Remember that I prefer dark mode in all applications"
â†’ add_memory extracts: preference about UI settings
```

**Recall context:**
```
"What do you know about my coding preferences?"
â†’ search_by_concept(concept_type="preference") 
â†’ Returns: dark mode preference, editor settings, etc.
```

**Find reasoning chains:**
```
"Why did we decide to use Rust for this project?"
â†’ search_reasoning_chain(chain_mode="causal")
â†’ Returns: decision â†’ because â†’ performance requirements
```

**Quick session context:**
```
"What were we working on?"
â†’ search_memory(mode="recent") 
â†’ Returns: last 4 hours of activity
```

**Complex reasoning with FastThink:**
```
"Let me think through this architecture decision..."
â†’ think_start(session_id="arch_decision")
â†’ think_add("Option A: microservices...")
â†’ think_add("Option B: monolith...")
â†’ think_recall("previous architecture decisions")  // pulls from main memory
â†’ think_conclude("Microservices because of scaling requirements")
â†’ think_commit()  // saves conclusion to persistent memory
```

---

## ðŸ§  FastThink (Working Memory)

FastThink provides **isolated scratchpad memory** for complex reasoning tasks. Think of it as a whiteboard that doesn't pollute your main memory until you're ready to commit.

### Why FastThink?

| Problem | Solution |
|---------|----------|
| Thinking out loud pollutes memory | Isolated session, commit only conclusions |
| Need to recall facts while thinking | `think_recall` reads main memory (read-only) |
| Analysis paralysis | Built-in limits: max thoughts, timeout, depth |
| Lost train of thought | Graph structure preserves reasoning chain |

### Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ think_start â”‚ â”€â”€â–¶ â”‚  think_add  â”‚ â”€â”€â–¶ â”‚think_recall â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  (repeat)   â”‚     â”‚ (optional)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                   â”‚
                           â–¼                   â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚think_concludeâ”‚ â”€â”€â–¶ â”‚think_commit â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                   â”‚
                           â–¼                   â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Saved to main
                    â”‚think_discardâ”‚     memory as fact
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Limits (configurable)

| Limit | Default | Purpose |
|-------|:-------:|---------|
| `max_thoughts` | 100 | Prevent infinite loops |
| `max_depth` | 10 | Limit reasoning depth |
| `thinking_timeout` | 30s | Prevent stuck sessions |
| `session_ttl` | 5min | Auto-cleanup stale sessions |

### Timeout Recovery

If a session times out, **partial thoughts are automatically saved** to main memory with `[INCOMPLETE]` marker:

```
â° Timeout detected
    â†“
ðŸ“ Thoughts saved with [INCOMPLETE] marker
    â†“
ðŸ’¾ Stored in main memory
    â†“
ðŸ” Found at next session start via search_memory("[INCOMPLETE]")
    â†“
ðŸ”„ Continue research or dismiss
```

**Recovery flow:**
1. At session start, AI searches for `[INCOMPLETE]` memories
2. If found, offers to continue the research
3. New FastThink session pulls context via `think_recall`
4. After completion, `update_memory` removes `[INCOMPLETE]` marker

No work is lost â€” incomplete reasoning becomes a starting point for next session.

---

## ðŸ“Š Search Modes

| Mode | Time Window | Graph Depth | Use Case |
|------|:-----------:|:-----------:|----------|
| `recent` | 4 hours | 1 | Current session context |
| `contextual` | 30 days | 2 | Balanced (default) |
| `deep` | 90 days | 3 | Thorough historical search |
| `full` | All time | 4 | Complete memory archive |

---

## âš™ï¸ Configuration

### Environment Variables

| Variable | Required | Default | Description |
|----------|:--------:|---------|-------------|
| `HELIX_HOST` | âœ… | `localhost` | HelixDB server address |
| `HELIX_PORT` | âœ… | `6969` | HelixDB port |
| `HELIX_LLM_API_KEY` | âœ… | â€” | API key for LLM provider |
| `HELIX_EMBEDDING_API_KEY` | âœ… | â€” | API key for embeddings |
| `HELIX_LLM_PROVIDER` | | `cerebras` | `cerebras`, `openai`, `ollama` |
| `HELIX_LLM_MODEL` | | `llama-3.3-70b` | Model name |
| `HELIX_LLM_BASE_URL` | | â€” | Custom endpoint (Ollama) |
| `HELIX_EMBEDDING_PROVIDER` | | `openai` | `openai`, `ollama` |
| `HELIX_EMBEDDING_URL` | | `https://openrouter.ai/api/v1` | Embedding API URL |
| `HELIX_EMBEDDING_MODEL` | | `all-mpnet-base-v2` | Embedding model |

### Provider Configurations

#### Option 1: Cerebras + OpenRouter (Recommended)

Ultra-fast inference + cheap embeddings:

```bash
HELIX_LLM_PROVIDER=cerebras
HELIX_LLM_MODEL=llama-3.3-70b
HELIX_LLM_API_KEY=csk-xxx              # https://cloud.cerebras.ai

HELIX_EMBEDDING_PROVIDER=openai
HELIX_EMBEDDING_URL=https://openrouter.ai/api/v1
HELIX_EMBEDDING_MODEL=openai/text-embedding-3-large
HELIX_EMBEDDING_API_KEY=sk-or-xxx      # https://openrouter.ai/keys
```

#### Option 2: Fully Local (Ollama)

No API keys, fully private:

```bash
# Install Ollama first: curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3:8b
ollama pull nomic-embed-text

HELIX_LLM_PROVIDER=ollama
HELIX_LLM_MODEL=llama3:8b
HELIX_LLM_BASE_URL=http://localhost:11434

HELIX_EMBEDDING_PROVIDER=ollama
HELIX_EMBEDDING_URL=http://localhost:11434
HELIX_EMBEDDING_MODEL=nomic-embed-text
```

#### Option 3: OpenAI Only

Simple setup, one API key:

```bash
HELIX_LLM_PROVIDER=openai
HELIX_LLM_MODEL=gpt-4o-mini
HELIX_LLM_API_KEY=sk-xxx

HELIX_EMBEDDING_PROVIDER=openai
HELIX_EMBEDDING_MODEL=text-embedding-3-small
HELIX_EMBEDDING_API_KEY=sk-xxx
```

---

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MCP Server (stdio)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      HelixirClient                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      ToolingManager       â”‚        FastThinkManager         â”‚
â”‚                           â”‚     (in-memory scratchpad)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LLM      â”‚Decisionâ”‚Entity â”‚  petgraph::StableDiGraph        â”‚
â”‚ Extractorâ”‚ Engine â”‚Managerâ”‚  (thoughts, entities, concepts) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Reasoningâ”‚ Search â”‚Ontologyâ”‚         â†“ commit               â”‚
â”‚ Engine   â”‚ Engine â”‚Manager â”‚         â†“                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      HelixDB Client                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   HelixDB (graph + vector)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ³ Docker

### Full Stack (HelixDB + Helixir)

```bash
# Start everything
docker-compose up -d

# Check logs
docker-compose logs -f helixir-mcp
```

### Standalone

```bash
# Build
docker build -t helixir-rs .

# Run with external HelixDB
docker run -e HELIX_HOST=your_helixdb_host \
           -e LLM_API_KEY=xxx \
           -e EMBEDDING_API_KEY=xxx \
           helixir-rs
```

---

## ðŸ§ª Development

```bash
# Run tests
cargo test

# Verbose logging
RUST_LOG=helixir=debug cargo run --bin helixir-mcp

# Lint
cargo clippy
cargo fmt --check
```

---

## ðŸ“„ License

[AGPL-3.0-or-later](LICENSE)

âš ï¸ **This is NOT MIT!** If you modify and deploy Helixir as a service, you must open-source your codebase.

---

## ðŸ”— Links

- [HelixDB](https://github.com/HelixDB/helix-db) â€” Graph-vector database
- [Helixir-Py](https://github.com/nikita-rulenko/helixir-py) â€” Python prototype (deprecated)
- [MCP Specification](https://spec.modelcontextprotocol.io/) â€” Model Context Protocol
- [Cerebras](https://cloud.cerebras.ai) â€” Fast LLM inference (free tier)
- [OpenRouter](https://openrouter.ai) â€” Unified LLM/embedding API
